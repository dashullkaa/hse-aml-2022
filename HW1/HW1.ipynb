{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "useful-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be73ab7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indonesian-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb52337",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use `HashingVectorizer` to encode the text into sparse features.\n",
    "\n",
    "I found out that in English there are about 1M unique words, so default n_features=2**20 will be enough.\n",
    "Also, I added stop_words parameter to exclude common words from corpus, because they are not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "retired-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(n_features=2**20, binary=True, stop_words={'english'})\n",
    "transformed_texts = hv.fit_transform(data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabc2be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the K-Fold cross-validation to split the dataset into training and test parts.\n",
    "\n",
    "Dataset will be split into 5 parts, the data will be shuffled before and to make the output reproducible I set random_state parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "declared-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acaba51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use Logistic Regression to create a model.\n",
    "\n",
    "I used default parameters to get first results. To compute cross-validated metrics I used cross_val_score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "transparent-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=1e-5, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "periodic-charleston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.89262042, 0.89703933, 0.87892179, 0.8935042 , 0.90318302])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, transformed_texts, data.target, cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52925f20",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I got pretty good results, and now I want to experiment with different parameters (regularization/loss function/alpha). This experiment will help to find the best parameters.\n",
    "\n",
    "Also, I decided to use GridSearchCV class to reduce the amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sunrise-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['hinge', 'log'],\n",
    "    'alpha': [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "clf_search = GridSearchCV(clf, params, cv=kf, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "necessary-platinum",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l1; total time=   2.5s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l1; total time=   2.3s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l1; total time=   2.2s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l1; total time=   2.2s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l1; total time=   2.8s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l2; total time=   0.9s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l2; total time=   0.9s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l2; total time=   0.9s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l2; total time=   0.7s\n",
      "[CV] END ................alpha=1e-06, loss=hinge, penalty=l2; total time=   0.8s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l1; total time=   2.1s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l1; total time=   2.3s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l1; total time=   2.5s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l1; total time=   2.4s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l1; total time=   2.3s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l2; total time=   0.8s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l2; total time=   0.9s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l2; total time=   1.0s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l2; total time=   0.9s\n",
      "[CV] END ..................alpha=1e-06, loss=log, penalty=l2; total time=   0.8s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l1; total time=   1.9s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l1; total time=   1.8s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l1; total time=   2.0s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l1; total time=   1.8s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l1; total time=   1.8s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l2; total time=   0.7s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l2; total time=   0.8s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l2; total time=   0.7s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l2; total time=   0.6s\n",
      "[CV] END ................alpha=1e-05, loss=hinge, penalty=l2; total time=   0.7s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l1; total time=   2.3s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l1; total time=   2.3s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l1; total time=   2.6s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l1; total time=   2.9s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l1; total time=   2.8s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l2; total time=   0.9s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l2; total time=   0.9s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l2; total time=   1.0s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l2; total time=   1.1s\n",
      "[CV] END ..................alpha=1e-05, loss=log, penalty=l2; total time=   1.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   2.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.7s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   2.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.7s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.6s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.9s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.8s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.9s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.8s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.7s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   2.7s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   2.2s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   2.7s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   2.9s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   2.6s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.8s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.9s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.7s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.7s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.7s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.2s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.2s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.3s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.3s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.6s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.6s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.6s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.6s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.6s\n",
      "[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.5s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.9s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.9s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   2.4s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   2.0s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.9s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.7s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.7s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s\n",
      "[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=KFold(n_splits=5, random_state=7, shuffle=True),\n             estimator=SGDClassifier(alpha=1e-05, loss='log', n_jobs=-1,\n                                     random_state=42),\n             param_grid={'alpha': [1e-06, 1e-05, 0.0001, 0.001],\n                         'loss': ['hinge', 'log'], 'penalty': ['l1', 'l2']},\n             verbose=2)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_search.fit(transformed_texts, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "taken-presentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'alpha': 1e-05, 'loss': 'hinge', 'penalty': 'l2'} \n",
      "best_score: 0.8982685753557498\n"
     ]
    }
   ],
   "source": [
    "print('best_params:', clf_search.best_params_, '\\nbest_score:', clf_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee3bd0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results obtained above are explainable.\n",
    "\n",
    "Since we have a classification problem, `hinge loss` function works better than log, because log function is useful when we are trying to estimate probability. Hinge loss leads to better accuracy at the cost of much less sensitivity regarding probabilities.\n",
    "\n",
    "`L2 penalty` leads to minimizing all model weights, which makes it more robust. We should use L1 regularization when we are trying to decrease influence of some features, that we have done before, when we exclude stop words.\n",
    "\n",
    "`Alpha` parameter is a multiplier for regularization term, and it means how big steps we make towards the function minimum. It is always chosen experimentally and depends on dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brown-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pretty-executive",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   param_alpha param_loss param_penalty  mean_test_score\n0     0.000001      hinge            l1         0.871310\n1     0.000001      hinge            l2         0.880327\n2     0.000001        log            l1         0.882536\n3     0.000001        log            l2         0.897208\n4      0.00001      hinge            l1         0.863621\n5      0.00001      hinge            l2         0.898269\n6      0.00001        log            l1         0.874316\n7      0.00001        log            l2         0.893054\n8       0.0001      hinge            l1         0.813948\n9       0.0001      hinge            l2         0.897473\n10      0.0001        log            l1         0.772053\n11      0.0001        log            l2         0.846739\n12       0.001      hinge            l1         0.369810\n13       0.001      hinge            l2         0.859909\n14       0.001        log            l1         0.351511\n15       0.001        log            l2         0.717961",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>param_alpha</th>\n      <th>param_loss</th>\n      <th>param_penalty</th>\n      <th>mean_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.871310</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.880327</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.882536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.897208</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.863621</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.898269</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.00001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.874316</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.893054</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.813948</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.897473</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.0001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.772053</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.0001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.846739</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.369810</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.859909</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.351511</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.717961</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf_search.cv_results_)[['param_alpha', 'param_loss', 'param_penalty', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63835133",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data in this table confirm the conclusion made above. Alpha less than 1e-5 is too big and mess the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}